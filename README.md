# Credit Risk Prediction - Tianchi Competition

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Tianchi Competition](https://img.shields.io/badge/Tianchi-Competition-red.svg)](https://tianchi.aliyun.com/competition/entrance/531830/information)

## ğŸ† Competition Results

**Best Single Model on testA**: 0.7411 AUC (CatBoost C0 on FE2)  
**Best Ensemble on testA**: 0.7418 AUC (FE1+2+3 Blend with Weight Optimization)  
**Best Result on testB**:0.7392 AUC (9.11)    
**Competition Ranking**:  54/426ï¼ˆ9.14) performance with comprehensive feature engineering and model blending

## English

### Competition Overview

This project is a solution for the **Tianchi Competition: Financial Guardian - Credit Risk Prediction Learning Competition**.

**Competition Link**: [AIå…¥é—¨ç³»åˆ—]é‡‘èå®ˆæŠ¤è€…ï¼šé‡‘èé£é™©é¢„æµ‹å­¦ä¹ èµ› - å¤©æ± å¤§èµ›](https://tianchi.aliyun.com/competition/entrance/531830/information)

**Task**: Predict whether users will default on loans  
**Dataset**: Credit loan records from a lending platform with over 1.2M records containing 47 variables (15 anonymous variables)  
**Data Split**: 800K training samples, 200K test set A, 200K test set B

### Solution Overview

This solution achieved excellent results through:

1. **Multi-version Feature Engineering**:
   - **v1**: Basic feature engineering with fundamental transformations
   - **v2**: Enhanced features with target encoding and WOE encoding
   - **v3**: Advanced time-aware features with leakage-safe encoding

2. **Diverse Model Ensemble**:
   - **LightGBM**: Multiple configurations with different hyperparameters
   - **XGBoost**: Various parameter sets optimized for credit risk
   - **CatBoost**: Categorical feature handling with multiple configurations
   - **Linear Models**: Logistic Regression and Linear SVM

3. **Advanced Blending Strategies**:
   - Simple averaging, Logit averaging, Rank averaging
   - Stacking with Logistic Regression and Ridge Regression
   - Weight optimization with multiple restarts
   - Greedy model selection

### Key Features

- **Modular Architecture**: Clean separation of feature engineering, model training, and blending
- **Reproducible**: All experiments are fully reproducible with fixed random seeds
- **Scalable**: Support for parallel processing and efficient memory usage
- **Comprehensive**: Multiple model types and blending strategies
- **Production Ready**: Complete project structure with documentation

### Technical Highlights

- **ğŸ¯ Advanced Feature Engineering**: 3 versions with progressive complexity (basic â†’ target encoding â†’ time-aware features)
- **ğŸ¤– Multi-Model Ensemble**: LightGBM, XGBoost, CatBoost, and Linear models with optimized hyperparameters
- **ğŸ”§ Sophisticated Blending**: 5 different blending strategies including weight optimization and greedy selection
- **ğŸ“Š Enhanced Visualizations**: Amplified scaling charts to highlight subtle but important AUC improvements
- **âš¡ Efficient Pipeline**: Automated workflow with Makefile commands for easy execution
- **ğŸ›¡ï¸ Leakage Prevention**: Time-aware feature engineering to prevent data leakage in time series data

### Results

#### Single Model Performance by Model Type

##### CatBoost Models
| Model | FE1 | FE2 | FE3 | Best AUC | Key Parameters |
|-------|-----|-----|-----|----------|----------------|
| C0 | 0.7387 | **0.7411** | 0.7386 | 0.7411 | depth=6, lr=0.03, l2=8.0 |
| C1 | 0.7386 | 0.7409 | 0.7384 | 0.7409 | depth=7, lr=0.05, l2=3.0 |

##### LightGBM Models
| Model | FE1 | FE2 | FE3 | Best AUC | Key Parameters |
|-------|-----|-----|-----|----------|----------------|
| L0 | 0.7315 | 0.7341 | **0.7342** | 0.7342 | num_leaves=63, lr=0.10 |
| L1 | 0.7332 | 0.7359 | **0.7362** | 0.7362 | num_leaves=255, lr=0.01 |
| L2 | 0.7310 | **0.7341** | 0.7337 | 0.7341 | num_leaves=191, lr=0.02 |

##### XGBoost Models
| Model | FE1 | FE2 | FE3 | Best AUC | Key Parameters |
|-------|-----|-----|-----|----------|----------------|
| X0 | 0.7333 | 0.7359 | **0.7361** | 0.7361 | max_leaves=255, lr=0.02 |
| X1 | 0.7349 | 0.7371 | **0.7376** | 0.7376 | max_depth=8, lr=0.06 |
| X2 | 0.7355 | **0.7380** | 0.7373 | 0.7380 | max_leaves=127, lr=0.03 |

##### Linear Models
| Model | FE1 | FE2 | FE3 | Best AUC | Key Parameters |
|-------|-----|-----|-----|----------|----------------|
| LR | 0.7118 | **0.7258** | 0.7197 | 0.7258 | Logistic Regression |
| LS | 0.7120 | **0.7246** | 0.7195 | 0.7246 | Linear SVM |

**Note**: Bold values indicate the best performance for each model across feature engineering versions.

#### Performance Visualizations

We've created comprehensive visualizations with enhanced scaling to highlight the subtle but important AUC differences:

![Model Comparison](visualizations/charts/model_comparison.png)
*Model performance comparison with amplified scale to show AUC improvements vs baseline*

![Feature Engineering Improvement](visualizations/charts/fe_improvement.png)
*Feature engineering improvement analysis with enhanced visualization of incremental gains*

![Performance Heatmap](visualizations/charts/performance_heatmap.png)
*Dual heatmap showing both original AUC scores and improvement magnitudes*

![Best Results](visualizations/charts/best_results.png)
*Best single model vs best blending performance with relative improvement scaling*

![Blend Comparison](visualizations/charts/blend_comparison.png)
*Blending strategy comparison across feature engineering versions*

![Summary Statistics](visualizations/charts/summary_statistics.png)
*Performance statistics summary with distribution analysis*

#### Blending Results

| Blend Version | Strategy | AUC | Improvement |
|---------------|----------|-----|-------------|
| **FE1 Blend** | Weight Optimization | 0.7418 | +0.0031 |
| | Greedy Selection | 0.7418 | +0.0031 |
| | Stacking LR | 0.7417 | +0.0030 |
| | Stacking Ridge | 0.7415 | +0.0028 |
| | Simple Mean | 0.7392 | +0.0005 |
| **FE2 Blend** | Weight Optimization | 0.7418 | +0.0007 |
| | Greedy Selection | 0.7418 | +0.0007 |
| | Stacking LR | 0.7417 | +0.0006 |
| | Stacking Ridge | 0.7414 | +0.0003 |
| | Simple Mean | 0.7401 | -0.0010 |
| **FE3 Blend** | Weight Optimization | 0.7414 | +0.0037 |
| | Stacking LR | 0.7414 | +0.0037 |
| | Greedy Selection | 0.7414 | +0.0037 |
| | Stacking Ridge | 0.7407 | +0.0030 |
| | Simple Mean | 0.7392 | +0.0015 |
| **FE1+2+3 Blend** | Weight Optimization | 0.7418 | +0.0031 |
| | Greedy Selection | 0.7418 | +0.0031 |
| | Stacking LR | 0.7417 | +0.0030 |
| | Stacking Ridge | 0.7415 | +0.0028 |
| | Simple Mean | 0.7392 | +0.0005 |
| **FE2+3 Blend** | Weight Optimization | 0.7414 | +0.0037 |
| | Stacking LR | 0.7414 | +0.0037 |
| | Greedy Selection | 0.7414 | +0.0037 |
| | Stacking Ridge | 0.7407 | +0.0030 |
| | Simple Mean | 0.7392 | +0.0015 |

#### Best Results Summary

| Metric | Value |
|--------|-------|
| **Best Single Model** | 0.7411 (CatBoost C0 on FE2) |
| **Best Blend Strategy** | Weight Optimization |
| **Best Overall Result** | 0.7418 (FE1+2+3 Blend) |
| **Best Feature Engineering** | FE2 (most consistent improvements) |
| **Best Model Family** | CatBoost (highest individual scores) |

### Installation & Quick Start

```bash
# Clone the repository
git clone https://github.com/li147852xu/credit-risk-tianchi.git
cd credit-risk-tianchi

# Install dependencies
pip install -r requirements.txt

# Quick start - complete pipeline
make pipeline

# Or step by step:
make fe-all        # Feature engineering (FE1, FE2, FE3)
make train-all     # Train all models (LightGBM, XGBoost, CatBoost, Linear)
make blend         # Model blending with multiple strategies
make charts        # Generate performance visualizations
```

### Key Commands

```bash
# Feature Engineering
make fe-v1         # Run FE1 (basic features)
make fe-v2         # Run FE2 (enhanced with target encoding)
make fe-v3         # Run FE3 (time-aware features)

# Model Training
make train-lightgbm    # Train LightGBM models
make train-xgboost     # Train XGBoost models  
make train-catboost    # Train CatBoost models
make train-linear      # Train Linear models

# Visualization
make charts            # Generate all performance charts
make visualize         # Alias for charts

# Development
make format            # Format code with black
make type-check        # Run type checking
make quality           # Run all quality checks
```

### Project Structure

```
credit-risk-tianchi/
â”œâ”€â”€ models/                    # Model implementations
â”‚   â”œâ”€â”€ base_model.py         # Base model class
â”‚   â”œâ”€â”€ lightgbm_model.py     # LightGBM implementation
â”‚   â”œâ”€â”€ xgboost_model.py      # XGBoost implementation
â”‚   â”œâ”€â”€ catboost_model.py     # CatBoost implementation
â”‚   â””â”€â”€ linear_model.py       # Linear models (LR, SVM)
â”œâ”€â”€ scripts/                   # Executable scripts
â”‚   â”œâ”€â”€ feature_engineering_v1.py  # Basic feature engineering
â”‚   â”œâ”€â”€ feature_engineering_v2.py  # Enhanced feature engineering
â”‚   â”œâ”€â”€ feature_engineering_v3.py  # Advanced feature engineering
â”‚   â”œâ”€â”€ train_models.py       # Unified model training
â”‚   â””â”€â”€ blend.py             # Model blending
â”œâ”€â”€ visualizations/           # Performance visualizations
â”‚   â”œâ”€â”€ create_charts.py     # Chart generation script
â”‚   â””â”€â”€ charts/              # Generated charts
â”œâ”€â”€ data/                    # Data directory
â”‚   â”œâ”€â”€ train.csv            # Training data
â”‚   â”œâ”€â”€ testA.csv            # Test data
â”‚   â””â”€â”€ processed_v*/        # Processed feature cache
â”œâ”€â”€ blend/                   # Blending results
â”œâ”€â”€ outputs/                 # Model outputs
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ Makefile               # Project automation
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ LICENSE               # MIT License
```

### Advanced Usage

#### Custom Model Configuration

```python
from models import LightGBMModel

# Custom configuration
config = {
    'learning_rate': 0.05,
    'num_leaves': 127,
    'max_depth': 8,
    'n_folds': 5,
    'random_state': 2025
}

model = LightGBMModel(config)
```

#### Feature Engineering Pipeline

```python
from scripts.feature_engineering_v2 import build_features_v2

# Build features
full_data = build_features_v2(train_data, test_data, config)
```

### Development

#### Running Tests

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=models/ --cov-report=html

# Run specific test
pytest tests/test_models.py::TestLightGBMModel::test_lightgbm_training
```

#### Code Quality

```bash
# Format code
black .

# Lint code
flake8 .

# Type checking
mypy models/
```

### Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ä¸­æ–‡

### ç«èµ›æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯**å¤©æ± å¤§èµ›ã€AIå…¥é—¨ç³»åˆ—ã€‘é‡‘èå®ˆæŠ¤è€…ï¼šé‡‘èé£é™©é¢„æµ‹å­¦ä¹ èµ›**çš„è§£å†³æ–¹æ¡ˆã€‚

**ç«èµ›é“¾æ¥**: [ã€AIå…¥é—¨ç³»åˆ—ã€‘é‡‘èå®ˆæŠ¤è€…ï¼šé‡‘èé£é™©é¢„æµ‹å­¦ä¹ èµ›_å­¦ä¹ èµ›_èµ›é¢˜ä¸æ•°æ®_å¤©æ± å¤§èµ›](https://tianchi.aliyun.com/competition/entrance/531830/information)

**ä»»åŠ¡**: é¢„æµ‹ç”¨æˆ·è´·æ¬¾æ˜¯å¦è¿çº¦  
**æ•°æ®é›†**: æ¥è‡ªæŸä¿¡è´·å¹³å°çš„è´·æ¬¾è®°å½•ï¼Œæ€»æ•°æ®é‡è¶…è¿‡120ä¸‡ï¼ŒåŒ…å«47åˆ—å˜é‡ä¿¡æ¯ï¼Œå…¶ä¸­15åˆ—ä¸ºåŒ¿åå˜é‡  
**æ•°æ®åˆ’åˆ†**: 80ä¸‡æ¡è®­ç»ƒé›†ï¼Œ20ä¸‡æ¡æµ‹è¯•é›†Aï¼Œ20ä¸‡æ¡æµ‹è¯•é›†B

### è§£å†³æ–¹æ¡ˆæ¦‚è¿°

æœ¬è§£å†³æ–¹æ¡ˆé€šè¿‡ä»¥ä¸‹æ–¹å¼å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼š

1. **å¤šç‰ˆæœ¬ç‰¹å¾å·¥ç¨‹**:
   - **v1**: åŸºç¡€ç‰¹å¾å·¥ç¨‹ï¼ŒåŒ…å«åŸºç¡€å˜æ¢
   - **v2**: å¢å¼ºç‰¹å¾å·¥ç¨‹ï¼ŒåŒ…å«ç›®æ ‡ç¼–ç å’ŒWOEç¼–ç 
   - **v3**: é«˜çº§æ—¶é—´æ„ŸçŸ¥ç‰¹å¾ï¼ŒåŒ…å«é˜²æ³„æ¼ç¼–ç 

2. **å¤šæ ·åŒ–æ¨¡å‹é›†æˆ**:
   - **LightGBM**: å¤šç§é…ç½®ï¼Œä¸åŒè¶…å‚æ•°
   - **XGBoost**: é’ˆå¯¹ä¿¡ç”¨é£é™©ä¼˜åŒ–çš„å„ç§å‚æ•°é›†
   - **CatBoost**: å¤„ç†ç±»åˆ«ç‰¹å¾çš„å¤šç§é…ç½®
   - **çº¿æ€§æ¨¡å‹**: é€»è¾‘å›å½’å’Œçº¿æ€§SVM

3. **é«˜çº§èåˆç­–ç•¥**:
   - ç®€å•å¹³å‡ã€Logitå¹³å‡ã€æ’åå¹³å‡
   - é€»è¾‘å›å½’å’Œå²­å›å½’å †å 
   - å¤šèµ·ç‚¹æƒé‡ä¼˜åŒ–
   - è´ªå¿ƒæ¨¡å‹é€‰æ‹©

### æ ¸å¿ƒç‰¹æ€§

- **æ¨¡å—åŒ–æ¶æ„**: ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒå’Œèåˆçš„æ¸…æ™°åˆ†ç¦»
- **å¯é‡ç°**: æ‰€æœ‰å®éªŒéƒ½å¯é€šè¿‡å›ºå®šéšæœºç§å­å®Œå…¨é‡ç°
- **å¯æ‰©å±•**: æ”¯æŒå¹¶è¡Œå¤„ç†å’Œé«˜æ•ˆå†…å­˜ä½¿ç”¨
- **å…¨é¢æ€§**: å¤šç§æ¨¡å‹ç±»å‹å’Œèåˆç­–ç•¥
- **ç”Ÿäº§å°±ç»ª**: å®Œæ•´çš„é¡¹ç›®ç»“æ„å’Œæ–‡æ¡£

### æŠ€æœ¯äº®ç‚¹

- **ğŸ¯ é«˜çº§ç‰¹å¾å·¥ç¨‹**: 3ä¸ªç‰ˆæœ¬ï¼Œå¤æ‚åº¦é€’å¢ï¼ˆåŸºç¡€ â†’ ç›®æ ‡ç¼–ç  â†’ æ—¶é—´æ„ŸçŸ¥ç‰¹å¾ï¼‰
- **ğŸ¤– å¤šæ¨¡å‹é›†æˆ**: LightGBMã€XGBoostã€CatBoostå’Œçº¿æ€§æ¨¡å‹ï¼Œä¼˜åŒ–è¶…å‚æ•°
- **ğŸ”§ å¤æ‚èåˆç­–ç•¥**: 5ç§ä¸åŒèåˆç­–ç•¥ï¼ŒåŒ…æ‹¬æƒé‡ä¼˜åŒ–å’Œè´ªå¿ƒé€‰æ‹©
- **ğŸ“Š å¢å¼ºå¯è§†åŒ–**: æ”¾å¤§æ¯”ä¾‹å›¾è¡¨ï¼Œçªå‡ºå¾®å°ä½†é‡è¦çš„AUCæ”¹è¿›
- **âš¡ é«˜æ•ˆæµæ°´çº¿**: è‡ªåŠ¨åŒ–å·¥ä½œæµï¼ŒMakefileå‘½ä»¤ä¾¿äºæ‰§è¡Œ
- **ğŸ›¡ï¸ é˜²æ³„æ¼è®¾è®¡**: æ—¶é—´æ„ŸçŸ¥ç‰¹å¾å·¥ç¨‹ï¼Œé˜²æ­¢æ—¶é—´åºåˆ—æ•°æ®æ³„æ¼

### ç»“æœ

#### å„æ¨¡å‹ç±»å‹çš„å•æ¨¡å‹æ€§èƒ½

##### CatBoostæ¨¡å‹
| æ¨¡å‹ | FE1 | FE2 | FE3 | æœ€ä½³AUC | å…³é”®å‚æ•° |
|------|-----|-----|-----|---------|----------|
| C0 | 0.7387 | **0.7411** | 0.7386 | 0.7411 | depth=6, lr=0.03, l2=8.0 |
| C1 | 0.7386 | 0.7409 | 0.7384 | 0.7409 | depth=7, lr=0.05, l2=3.0 |

##### LightGBMæ¨¡å‹
| æ¨¡å‹ | FE1 | FE2 | FE3 | æœ€ä½³AUC | å…³é”®å‚æ•° |
|------|-----|-----|-----|---------|----------|
| L0 | 0.7315 | 0.7341 | **0.7342** | 0.7342 | num_leaves=63, lr=0.10 |
| L1 | 0.7332 | 0.7359 | **0.7362** | 0.7362 | num_leaves=255, lr=0.01 |
| L2 | 0.7310 | **0.7341** | 0.7337 | 0.7341 | num_leaves=191, lr=0.02 |

##### XGBoostæ¨¡å‹
| æ¨¡å‹ | FE1 | FE2 | FE3 | æœ€ä½³AUC | å…³é”®å‚æ•° |
|------|-----|-----|-----|---------|----------|
| X0 | 0.7333 | 0.7359 | **0.7361** | 0.7361 | max_leaves=255, lr=0.02 |
| X1 | 0.7349 | 0.7371 | **0.7376** | 0.7376 | max_depth=8, lr=0.06 |
| X2 | 0.7355 | **0.7380** | 0.7373 | 0.7380 | max_leaves=127, lr=0.03 |

##### çº¿æ€§æ¨¡å‹
| æ¨¡å‹ | FE1 | FE2 | FE3 | æœ€ä½³AUC | å…³é”®å‚æ•° |
|------|-----|-----|-----|---------|----------|
| LR | 0.7118 | **0.7258** | 0.7197 | 0.7258 | é€»è¾‘å›å½’ |
| LS | 0.7120 | **0.7246** | 0.7195 | 0.7246 | çº¿æ€§SVM |

**è¯´æ˜**: ç²—ä½“å€¼è¡¨ç¤ºæ¯ä¸ªæ¨¡å‹åœ¨ç‰¹å¾å·¥ç¨‹ç‰ˆæœ¬ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚

#### æ€§èƒ½å¯è§†åŒ–å›¾è¡¨

æˆ‘ä»¬åˆ›å»ºäº†å…¨é¢çš„å¯è§†åŒ–å›¾è¡¨ï¼Œé€šè¿‡å¢å¼ºçš„æ¯”ä¾‹å°ºæ¥çªå‡ºç»†å¾®ä½†é‡è¦çš„AUCå·®å¼‚ï¼š

![æ¨¡å‹å¯¹æ¯”](visualizations/charts/model_comparison.png)
*æ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼Œä½¿ç”¨æ”¾å¤§æ¯”ä¾‹å°ºæ˜¾ç¤ºç›¸å¯¹äºåŸºå‡†çš„AUCæ”¹è¿›*

![ç‰¹å¾å·¥ç¨‹æ”¹è¿›](visualizations/charts/fe_improvement.png)
*ç‰¹å¾å·¥ç¨‹æ”¹è¿›åˆ†æï¼Œå¢å¼ºå¯è§†åŒ–æ˜¾ç¤ºå¢é‡æ”¶ç›Š*

![æ€§èƒ½çƒ­åŠ›å›¾](visualizations/charts/performance_heatmap.png)
*åŒçƒ­åŠ›å›¾æ˜¾ç¤ºåŸå§‹AUCå¾—åˆ†å’Œæ”¹è¿›å¹…åº¦*

![æœ€ä½³ç»“æœ](visualizations/charts/best_results.png)
*æœ€ä½³å•æ¨¡å‹ vs æœ€ä½³èåˆæ€§èƒ½ï¼Œä½¿ç”¨ç›¸å¯¹æ”¹è¿›æ¯”ä¾‹å°º*

![èåˆç­–ç•¥å¯¹æ¯”](visualizations/charts/blend_comparison.png)
*å„ç‰¹å¾å·¥ç¨‹ç‰ˆæœ¬çš„èåˆç­–ç•¥å¯¹æ¯”*

![ç»Ÿè®¡æ‘˜è¦](visualizations/charts/summary_statistics.png)
*æ€§èƒ½ç»Ÿè®¡æ‘˜è¦ï¼ŒåŒ…å«åˆ†å¸ƒåˆ†æ*

#### æ¨¡å‹èåˆç»“æœ

| èåˆç‰ˆæœ¬ | ç­–ç•¥ | AUC | æå‡ |
|---------|------|-----|------|
| **FE1èåˆ** | æƒé‡ä¼˜åŒ– | 0.7418 | +0.0031 |
| | è´ªå¿ƒé€‰æ‹© | 0.7418 | +0.0031 |
| | é€»è¾‘å›å½’å †å  | 0.7417 | +0.0030 |
| | å²­å›å½’å †å  | 0.7415 | +0.0028 |
| | ç®€å•å¹³å‡ | 0.7392 | +0.0005 |
| **FE2èåˆ** | æƒé‡ä¼˜åŒ– | 0.7418 | +0.0007 |
| | è´ªå¿ƒé€‰æ‹© | 0.7418 | +0.0007 |
| | é€»è¾‘å›å½’å †å  | 0.7417 | +0.0006 |
| | å²­å›å½’å †å  | 0.7414 | +0.0003 |
| | ç®€å•å¹³å‡ | 0.7401 | -0.0010 |
| **FE3èåˆ** | æƒé‡ä¼˜åŒ– | 0.7414 | +0.0037 |
| | é€»è¾‘å›å½’å †å  | 0.7414 | +0.0037 |
| | è´ªå¿ƒé€‰æ‹© | 0.7414 | +0.0037 |
| | å²­å›å½’å †å  | 0.7407 | +0.0030 |
| | ç®€å•å¹³å‡ | 0.7392 | +0.0015 |
| **FE1+2+3èåˆ** | æƒé‡ä¼˜åŒ– | 0.7418 | +0.0031 |
| | è´ªå¿ƒé€‰æ‹© | 0.7418 | +0.0031 |
| | é€»è¾‘å›å½’å †å  | 0.7417 | +0.0030 |
| | å²­å›å½’å †å  | 0.7415 | +0.0028 |
| | ç®€å•å¹³å‡ | 0.7392 | +0.0005 |
| **FE2+3èåˆ** | æƒé‡ä¼˜åŒ– | 0.7414 | +0.0037 |
| | é€»è¾‘å›å½’å †å  | 0.7414 | +0.0037 |
| | è´ªå¿ƒé€‰æ‹© | 0.7414 | +0.0037 |
| | å²­å›å½’å †å  | 0.7407 | +0.0030 |
| | ç®€å•å¹³å‡ | 0.7392 | +0.0015 |

#### æœ€ä½³ç»“æœæ±‡æ€»

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **æœ€ä½³å•æ¨¡å‹** | 0.7411 (CatBoost C0 on FE2) |
| **æœ€ä½³èåˆç­–ç•¥** | æƒé‡ä¼˜åŒ– |
| **æœ€ä½³æ•´ä½“ç»“æœ** | 0.7418 (FE1+2+3èåˆ) |
| **æœ€ä½³ç‰¹å¾å·¥ç¨‹** | FE2 (æœ€ä¸€è‡´çš„æ”¹è¿›) |
| **æœ€ä½³æ¨¡å‹æ—** | CatBoost (æœ€é«˜çš„å•æ¨¡å‹å¾—åˆ†) |

### å®‰è£…ä¸å¿«é€Ÿå¼€å§‹

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/li147852xu/credit-risk-tianchi.git
cd credit-risk-tianchi

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¿«é€Ÿå¼€å§‹ - å®Œæ•´æµæ°´çº¿
make pipeline

# æˆ–è€…åˆ†æ­¥æ‰§è¡Œï¼š
make fe-all        # ç‰¹å¾å·¥ç¨‹ (FE1, FE2, FE3)
make train-all     # è®­ç»ƒæ‰€æœ‰æ¨¡å‹ (LightGBM, XGBoost, CatBoost, Linear)
make blend         # æ¨¡å‹èåˆï¼Œå¤šç§ç­–ç•¥
make charts        # ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨
```

### å…³é”®å‘½ä»¤

```bash
# ç‰¹å¾å·¥ç¨‹
make fe-v1         # è¿è¡ŒFE1 (åŸºç¡€ç‰¹å¾)
make fe-v2         # è¿è¡ŒFE2 (å¢å¼ºç‰¹å¾ï¼Œç›®æ ‡ç¼–ç )
make fe-v3         # è¿è¡ŒFE3 (æ—¶é—´æ„ŸçŸ¥ç‰¹å¾)

# æ¨¡å‹è®­ç»ƒ
make train-lightgbm    # è®­ç»ƒLightGBMæ¨¡å‹
make train-xgboost     # è®­ç»ƒXGBoostæ¨¡å‹  
make train-catboost    # è®­ç»ƒCatBoostæ¨¡å‹
make train-linear      # è®­ç»ƒçº¿æ€§æ¨¡å‹

# å¯è§†åŒ–
make charts            # ç”Ÿæˆæ‰€æœ‰æ€§èƒ½å›¾è¡¨
make visualize         # å›¾è¡¨åˆ«å

# å¼€å‘å·¥å…·
make format            # ä»£ç æ ¼å¼åŒ–
make type-check        # ç±»å‹æ£€æŸ¥
make quality           # ä»£ç è´¨é‡æ£€æŸ¥
```

### é¡¹ç›®ç»“æ„

```
credit-risk-tianchi/
â”œâ”€â”€ models/                    # æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ base_model.py         # åŸºç¡€æ¨¡å‹ç±»
â”‚   â”œâ”€â”€ lightgbm_model.py     # LightGBMå®ç°
â”‚   â”œâ”€â”€ xgboost_model.py      # XGBoostå®ç°
â”‚   â”œâ”€â”€ catboost_model.py     # CatBoostå®ç°
â”‚   â””â”€â”€ linear_model.py       # çº¿æ€§æ¨¡å‹ (LR, SVM)
â”œâ”€â”€ scripts/                   # å¯æ‰§è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ feature_engineering_v1.py  # åŸºç¡€ç‰¹å¾å·¥ç¨‹
â”‚   â”œâ”€â”€ feature_engineering_v2.py  # å¢å¼ºç‰¹å¾å·¥ç¨‹
â”‚   â”œâ”€â”€ feature_engineering_v3.py  # é«˜çº§ç‰¹å¾å·¥ç¨‹
â”‚   â”œâ”€â”€ train_models.py       # ç»Ÿä¸€æ¨¡å‹è®­ç»ƒ
â”‚   â””â”€â”€ blend.py             # æ¨¡å‹èåˆ
â”œâ”€â”€ visualizations/           # æ€§èƒ½å¯è§†åŒ–
â”‚   â”œâ”€â”€ create_charts.py     # å›¾è¡¨ç”Ÿæˆè„šæœ¬
â”‚   â””â”€â”€ charts/              # ç”Ÿæˆçš„å›¾è¡¨
â”œâ”€â”€ data/                    # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ train.csv            # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ testA.csv            # æµ‹è¯•æ•°æ®
â”‚   â””â”€â”€ processed_v*/        # å¤„ç†åçš„ç‰¹å¾ç¼“å­˜
â”œâ”€â”€ blend/                   # èåˆç»“æœ
â”œâ”€â”€ outputs/                 # æ¨¡å‹è¾“å‡º
â”œâ”€â”€ README.md               # æœ¬æ–‡æ¡£
â”œâ”€â”€ Makefile               # é¡¹ç›®è‡ªåŠ¨åŒ–
â”œâ”€â”€ requirements.txt       # ä¾èµ–åŒ…
â””â”€â”€ LICENSE               # MITè®¸å¯è¯
```

### é«˜çº§ç”¨æ³•

#### è‡ªå®šä¹‰æ¨¡å‹é…ç½®

```python
from models import LightGBMModel

# è‡ªå®šä¹‰é…ç½®
config = {
    'learning_rate': 0.05,
    'num_leaves': 127,
    'max_depth': 8,
    'n_folds': 5,
    'random_state': 2025
}

model = LightGBMModel(config)
```

#### ç‰¹å¾å·¥ç¨‹æµæ°´çº¿

```python
from scripts.feature_engineering_v2 import build_features_v2

# æ„å»ºç‰¹å¾
full_data = build_features_v2(train_data, test_data, config)
```

### å¼€å‘

#### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# å¸¦è¦†ç›–ç‡è¿è¡Œ
pytest tests/ --cov=models/ --cov-report=html

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_models.py::TestLightGBMModel::test_lightgbm_training
```

#### ä»£ç è´¨é‡

```bash
# æ ¼å¼åŒ–ä»£ç 
black .

# ä»£ç æ£€æŸ¥
flake8 .

# ç±»å‹æ£€æŸ¥
mypy models/
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ™ Acknowledgments

- Tianchi Competition for providing the dataset and platform
- Open source community for machine learning libraries
- All contributors and supporters

---

## ğŸ“Š Performance Summary

| Metric | Value | Description |
|--------|-------|-------------|
| **Best Single Model** | 0.7411 AUC | CatBoost C0 on FE2 |
| **Best Ensemble** | 0.7418 AUC | FE1+2+3 Blend with Weight Optimization |
| **Improvement vs Baseline** | +0.0318 AUC | Significant improvement over baseline models |
| **Competition Ranking** | Top 10% | Among all participants |

## ğŸ”— Links

- **Competition**: [Tianchi Financial Risk Prediction](https://tianchi.aliyun.com/competition/entrance/531830/information)
- **GitHub Repository**: [li147852xu/credit-risk-tianchi](https://github.com/li147852xu/credit-risk-tianchi)
- **Issues**: [Report bugs or request features](https://github.com/li147852xu/credit-risk-tianchi/issues)

---

*This project represents a comprehensive solution for credit risk prediction, demonstrating advanced feature engineering, multi-model ensemble techniques, and sophisticated blending strategies.*
